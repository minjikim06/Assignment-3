{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "과제#4_G202038004_안현준.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNxJAhxavJuz",
        "outputId": "58763f2d-2807-4a9e-d166-262dbdb9c3a4"
      },
      "source": [
        "# If You use in Colab, You Should run this script\n",
        "import os\n",
        "if (not os.path.exists(\"./deep-learning-from-scratch-2\") and\n",
        "    not \"deep-learning-from-scratch-2\" in os.getcwd()):\n",
        "    !git clone  https://github.com/WegraLee/deep-learning-from-scratch-2.git\n",
        "    os.chdir(\"./deep-learning-from-scratch-2\")\n",
        "# !pip install wget"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-from-scratch-2'...\n",
            "remote: Enumerating objects: 598, done.\u001b[K\n",
            "remote: Total 598 (delta 0), reused 0 (delta 0), pack-reused 598\u001b[K\n",
            "Receiving objects: 100% (598/598), 29.81 MiB | 31.34 MiB/s, done.\n",
            "Resolving deltas: 100% (360/360), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYtQbMSjvLJz",
        "outputId": "14feda05-1b90-4452-f08b-afb430557e2c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at ./drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guvpuuz9vLLn",
        "outputId": "e70bb785-c78c-45f6-f266-5cda6bceafbc"
      },
      "source": [
        "cd \"/content/drive/MyDrive\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkKP8lmv5kW7"
      },
      "source": [
        "# Encoder 클래스 코드\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from common.base_model import BaseModel\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
        "\n",
        "        self.params = self.embed.params + self.lstm.params\n",
        "        self.grads = self.embed.grads + self.lstm.grads\n",
        "        self.hs = None\n",
        "\n",
        "    def forward(self, xs):\n",
        "        xs = self.embed.forward(xs)\n",
        "        hs = self.lstm.forward(xs)\n",
        "        self.hs = hs\n",
        "        return hs[:, -1, :]\n",
        "\n",
        "    def backward(self, dh):\n",
        "        dhs = np.zeros_like(self.hs)\n",
        "        dhs[:, -1, :] = dh\n",
        "\n",
        "        dout = self.lstm.backward(dhs)\n",
        "        dout = self.embed.backward(dout)\n",
        "        return dout\n",
        "\n",
        "# Decoder 클래스 코드\n",
        "class Decoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in (self.embed, self.lstm, self.affine):\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, xs, h):\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        out = self.lstm.forward(out)\n",
        "        score = self.affine.forward(out)\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        dout = self.affine.backward(dscore)\n",
        "        dout = self.lstm.backward(dout)\n",
        "        dout = self.embed.backward(dout)\n",
        "        dh = self.lstm.dh\n",
        "        return dh\n",
        "    \n",
        "    # Decoder 클래스에 문장 생성을 담당하는 generte() 메서드\n",
        "    def generate(self, h, start_id, sample_size):\n",
        "        sampled = []\n",
        "        sample_id = start_id\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array(sample_id).reshape((1, 1))\n",
        "            out = self.embed.forward(x)\n",
        "            out = self.lstm.forward(out)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            sample_id = np.argmax(score.flatten())\n",
        "            sampled.append(int(sample_id))\n",
        "\n",
        "        return sampled\n",
        "\n",
        "class Seq2seq(BaseModel):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = Encoder(V, D, H)\n",
        "        self.decoder = Decoder(V, D, H)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
        "\n",
        "        h = self.encoder.forward(xs)\n",
        "        score = self.decoder.forward(decoder_xs, h)\n",
        "        loss = self.softmax.forward(score, decoder_ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.softmax.backward(dout)\n",
        "        dh = self.decoder.backward(dout)\n",
        "        dout = self.encoder.backward(dh)\n",
        "        return dout\n",
        "\n",
        "    def generate(self, xs, start_id, sample_size):\n",
        "        h = self.encoder.forward(xs)\n",
        "        sampled = self.decoder.generate(h, start_id, sample_size)\n",
        "        return sampled\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YUh_AzT5kcl"
      },
      "source": [
        "class PeekyDecoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(H + D, 4 * H) / np.sqrt(H + D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in (self.embed, self.lstm, self.affine):\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, xs, h):\n",
        "        N, T = xs.shape\n",
        "        N, H = h.shape\n",
        "\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        hs = np.repeat(h, T, axis=0).reshape(N, T, H)\n",
        "        out = np.concatenate((hs, out), axis=2)\n",
        "\n",
        "        out = self.lstm.forward(out)\n",
        "        out = np.concatenate((hs, out), axis=2)\n",
        "\n",
        "        score = self.affine.forward(out)\n",
        "        self.cache = H\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        H = self.cache\n",
        "\n",
        "        dout = self.affine.backward(dscore)\n",
        "        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]\n",
        "        dout = self.lstm.backward(dout)\n",
        "        dembed, dhs1 = dout[:, :, H:], dout[:, :, :H]\n",
        "        self.embed.backward(dembed)\n",
        "\n",
        "        dhs = dhs0 + dhs1\n",
        "        dh = self.lstm.dh + np.sum(dhs, axis=1)\n",
        "        return dh\n",
        "\n",
        "    def generate(self, h, start_id, sample_size):\n",
        "        sampled = []\n",
        "        char_id = start_id\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        H = h.shape[1]\n",
        "        peeky_h = h.reshape(1, 1, H)\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array([char_id]).reshape((1, 1))\n",
        "            out = self.embed.forward(x)\n",
        "\n",
        "            out = np.concatenate((peeky_h, out), axis=2)\n",
        "            out = self.lstm.forward(out)\n",
        "            out = np.concatenate((peeky_h, out), axis=2)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            char_id = np.argmax(score.flatten())\n",
        "            sampled.append(char_id)\n",
        "\n",
        "        return sampled\n",
        "\n",
        "class PeekySeq2seq(Seq2seq):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = Encoder(V, D, H)\n",
        "        self.decoder = PeekyDecoder(V, D, H)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-CrQ7tk3XeW"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from ch07.seq2seq import Encoder, Seq2seq\n",
        "from ch08.attention_layer import TimeAttention\n",
        "\n",
        "\n",
        "class AttentionEncoder(Encoder):\n",
        "    def forward(self, xs):\n",
        "        xs = self.embed.forward(xs)\n",
        "        hs = self.lstm.forward(xs)\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        dout = self.lstm.backward(dhs)\n",
        "        dout = self.embed.backward(dout)\n",
        "        return dout\n",
        "\n",
        "\n",
        "class AttentionDecoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.attention = TimeAttention()\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, xs, enc_hs):\n",
        "        h = enc_hs[:,-1]\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        dec_hs = self.lstm.forward(out)\n",
        "        c = self.attention.forward(enc_hs, dec_hs)\n",
        "        out = np.concatenate((c, dec_hs), axis=2)\n",
        "        score = self.affine.forward(out)\n",
        "\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        dout = self.affine.backward(dscore)\n",
        "        N, T, H2 = dout.shape\n",
        "        H = H2 // 2\n",
        "\n",
        "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
        "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
        "        ddec_hs = ddec_hs0 + ddec_hs1\n",
        "        dout = self.lstm.backward(ddec_hs)\n",
        "        dh = self.lstm.dh\n",
        "        denc_hs[:, -1] += dh\n",
        "        self.embed.backward(dout)\n",
        "\n",
        "        return denc_hs\n",
        "\n",
        "    def generate(self, enc_hs, start_id, sample_size):\n",
        "        sampled = []\n",
        "        sample_id = start_id\n",
        "        h = enc_hs[:, -1]\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array([sample_id]).reshape((1, 1))\n",
        "\n",
        "            out = self.embed.forward(x)\n",
        "            dec_hs = self.lstm.forward(out)\n",
        "            c = self.attention.forward(enc_hs, dec_hs)\n",
        "            out = np.concatenate((c, dec_hs), axis=2)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            sample_id = np.argmax(score.flatten())\n",
        "            sampled.append(sample_id)\n",
        "\n",
        "        return sampled\n",
        "\n",
        "\n",
        "class AttentionSeq2seq(Seq2seq):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        args = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = AttentionEncoder(*args)\n",
        "        self.decoder = AttentionDecoder(*args)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28ckYb9I4OJT",
        "outputId": "45351297-650c-46c3-8cd0-54be64ccfe04"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "sys.path.append('../ch07')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset import sequence\n",
        "from common.optimizer import Adam\n",
        "from common.trainer import Trainer\n",
        "from common.util import eval_seq2seq\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "# 입력 문장 반전\n",
        "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "vocab_size = len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 256\n",
        "batch_size = 128\n",
        "max_epoch = 10\n",
        "max_grad = 5.0\n",
        "\n",
        "#----------------------------------------------------------------#\n",
        "#model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "#model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "#----------------------------------------------------------------#\n",
        "\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "#----------------------------#\n",
        "#acc_list = []\n",
        "#acc_list_seq = []\n",
        "acc_list_peeky = []\n",
        "#----------------------------#\n",
        "for epoch in range(max_epoch):\n",
        "    trainer.fit(x_train, t_train, max_epoch=1,\n",
        "                batch_size=batch_size, max_grad=max_grad)\n",
        "\n",
        "    correct_num = 0\n",
        "    for i in range(len(x_test)):\n",
        "        question, correct = x_test[[i]], t_test[[i]]\n",
        "        verbose = i < 10\n",
        "        correct_num += eval_seq2seq(model, question, correct,\n",
        "                                    id_to_char, verbose, is_reverse=True)\n",
        "\n",
        "    acc = float(correct_num) / len(x_test)\n",
        "    #----------------------------#\n",
        "    #acc_list.append(acc)\n",
        "    #acc_list_seq.append(acc)\n",
        "    acc_list_peeky.append(acc)\n",
        "    #----------------------------#\n",
        "    print('정확도 %.3f%%' % (acc * 100))\n",
        "\n",
        "\n",
        "model.save_params()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
            "| 에폭 1 |  반복 21 / 351 | 시간 12[s] | 손실 2.86\n",
            "| 에폭 1 |  반복 41 / 351 | 시간 24[s] | 손실 1.89\n",
            "| 에폭 1 |  반복 61 / 351 | 시간 37[s] | 손실 1.78\n",
            "| 에폭 1 |  반복 81 / 351 | 시간 49[s] | 손실 1.70\n",
            "| 에폭 1 |  반복 101 / 351 | 시간 61[s] | 손실 1.57\n",
            "| 에폭 1 |  반복 121 / 351 | 시간 74[s] | 손실 1.30\n",
            "| 에폭 1 |  반복 141 / 351 | 시간 86[s] | 손실 1.16\n",
            "| 에폭 1 |  반복 161 / 351 | 시간 98[s] | 손실 1.10\n",
            "| 에폭 1 |  반복 181 / 351 | 시간 110[s] | 손실 1.07\n",
            "| 에폭 1 |  반복 201 / 351 | 시간 122[s] | 손실 1.05\n",
            "| 에폭 1 |  반복 221 / 351 | 시간 135[s] | 손실 1.04\n",
            "| 에폭 1 |  반복 241 / 351 | 시간 147[s] | 손실 1.04\n",
            "| 에폭 1 |  반복 261 / 351 | 시간 159[s] | 손실 1.03\n",
            "| 에폭 1 |  반복 281 / 351 | 시간 172[s] | 손실 1.02\n",
            "| 에폭 1 |  반복 301 / 351 | 시간 184[s] | 손실 1.01\n",
            "| 에폭 1 |  반복 321 / 351 | 시간 196[s] | 손실 1.00\n",
            "| 에폭 1 |  반복 341 / 351 | 시간 209[s] | 손실 1.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 1971-11-11\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 1973-01-11\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[91m☒\u001b[0m 1983-03-03\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[91m☒\u001b[0m 1973-01-11\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[91m☒\u001b[0m 1973-04-09\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[91m☒\u001b[0m 1971-11-11\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[91m☒\u001b[0m 1983-04-09\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 1983-04-09\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 1971-11-11\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 1996-01-11\n",
            "---\n",
            "정확도 0.020%\n",
            "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 2 |  반복 21 / 351 | 시간 13[s] | 손실 0.98\n",
            "| 에폭 2 |  반복 41 / 351 | 시간 25[s] | 손실 0.97\n",
            "| 에폭 2 |  반복 61 / 351 | 시간 37[s] | 손실 0.96\n",
            "| 에폭 2 |  반복 81 / 351 | 시간 50[s] | 손실 0.96\n",
            "| 에폭 2 |  반복 101 / 351 | 시간 63[s] | 손실 0.95\n",
            "| 에폭 2 |  반복 121 / 351 | 시간 75[s] | 손실 0.93\n",
            "| 에폭 2 |  반복 141 / 351 | 시간 88[s] | 손실 0.92\n",
            "| 에폭 2 |  반복 161 / 351 | 시간 100[s] | 손실 0.91\n",
            "| 에폭 2 |  반복 181 / 351 | 시간 113[s] | 손실 0.90\n",
            "| 에폭 2 |  반복 201 / 351 | 시간 125[s] | 손실 0.89\n",
            "| 에폭 2 |  반복 221 / 351 | 시간 138[s] | 손실 0.88\n",
            "| 에폭 2 |  반복 241 / 351 | 시간 150[s] | 손실 0.87\n",
            "| 에폭 2 |  반복 261 / 351 | 시간 162[s] | 손실 0.85\n",
            "| 에폭 2 |  반복 281 / 351 | 시간 175[s] | 손실 0.83\n",
            "| 에폭 2 |  반복 301 / 351 | 시간 188[s] | 손실 0.81\n",
            "| 에폭 2 |  반복 321 / 351 | 시간 200[s] | 손실 0.81\n",
            "| 에폭 2 |  반복 341 / 351 | 시간 213[s] | 손실 0.76\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 1974-11-04\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 2011-01-11\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[91m☒\u001b[0m 2003-03-03\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[91m☒\u001b[0m 2011-10-11\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[91m☒\u001b[0m 1971-09-11\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[91m☒\u001b[0m 2001-10-12\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[91m☒\u001b[0m 1992-04-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 1970-07-07\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 2006-10-27\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 2011-10-11\n",
            "---\n",
            "정확도 0.420%\n",
            "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 3 |  반복 21 / 351 | 시간 13[s] | 손실 0.74\n",
            "| 에폭 3 |  반복 41 / 351 | 시간 25[s] | 손실 0.71\n",
            "| 에폭 3 |  반복 61 / 351 | 시간 37[s] | 손실 0.69\n",
            "| 에폭 3 |  반복 81 / 351 | 시간 48[s] | 손실 0.67\n",
            "| 에폭 3 |  반복 101 / 351 | 시간 60[s] | 손실 0.63\n",
            "| 에폭 3 |  반복 121 / 351 | 시간 71[s] | 손실 0.60\n",
            "| 에폭 3 |  반복 141 / 351 | 시간 83[s] | 손실 0.59\n",
            "| 에폭 3 |  반복 161 / 351 | 시간 94[s] | 손실 0.55\n",
            "| 에폭 3 |  반복 181 / 351 | 시간 106[s] | 손실 0.53\n",
            "| 에폭 3 |  반복 201 / 351 | 시간 117[s] | 손실 0.50\n",
            "| 에폭 3 |  반복 221 / 351 | 시간 129[s] | 손실 0.48\n",
            "| 에폭 3 |  반복 241 / 351 | 시간 141[s] | 손실 0.45\n",
            "| 에폭 3 |  반복 261 / 351 | 시간 153[s] | 손실 0.42\n",
            "| 에폭 3 |  반복 281 / 351 | 시간 164[s] | 손실 0.40\n",
            "| 에폭 3 |  반복 301 / 351 | 시간 175[s] | 손실 0.37\n",
            "| 에폭 3 |  반복 321 / 351 | 시간 186[s] | 손실 0.34\n",
            "| 에폭 3 |  반복 341 / 351 | 시간 198[s] | 손실 0.31\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 1985-10-13\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[91m☒\u001b[0m 2016-10-21\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[91m☒\u001b[0m 1970-07-17\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[91m☒\u001b[0m 1980-08-20\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 2007-08-06\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 2015-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 28.980%\n",
            "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.29\n",
            "| 에폭 4 |  반복 21 / 351 | 시간 11[s] | 손실 0.27\n",
            "| 에폭 4 |  반복 41 / 351 | 시간 23[s] | 손실 0.24\n",
            "| 에폭 4 |  반복 61 / 351 | 시간 34[s] | 손실 0.23\n",
            "| 에폭 4 |  반복 81 / 351 | 시간 45[s] | 손실 0.21\n",
            "| 에폭 4 |  반복 101 / 351 | 시간 56[s] | 손실 0.19\n",
            "| 에폭 4 |  반복 121 / 351 | 시간 68[s] | 손실 0.17\n",
            "| 에폭 4 |  반복 141 / 351 | 시간 79[s] | 손실 0.15\n",
            "| 에폭 4 |  반복 161 / 351 | 시간 91[s] | 손실 0.13\n",
            "| 에폭 4 |  반복 181 / 351 | 시간 103[s] | 손실 0.12\n",
            "| 에폭 4 |  반복 201 / 351 | 시간 115[s] | 손실 0.12\n",
            "| 에폭 4 |  반복 221 / 351 | 시간 127[s] | 손실 0.10\n",
            "| 에폭 4 |  반복 241 / 351 | 시간 139[s] | 손실 0.09\n",
            "| 에폭 4 |  반복 261 / 351 | 시간 151[s] | 손실 0.08\n",
            "| 에폭 4 |  반복 281 / 351 | 시간 163[s] | 손실 0.07\n",
            "| 에폭 4 |  반복 301 / 351 | 시간 174[s] | 손실 0.07\n",
            "| 에폭 4 |  반복 321 / 351 | 시간 186[s] | 손실 0.06\n",
            "| 에폭 4 |  반복 341 / 351 | 시간 198[s] | 손실 0.06\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 92.480%\n",
            "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.05\n",
            "| 에폭 5 |  반복 21 / 351 | 시간 11[s] | 손실 0.04\n",
            "| 에폭 5 |  반복 41 / 351 | 시간 23[s] | 손실 0.04\n",
            "| 에폭 5 |  반복 61 / 351 | 시간 34[s] | 손실 0.03\n",
            "| 에폭 5 |  반복 81 / 351 | 시간 45[s] | 손실 0.03\n",
            "| 에폭 5 |  반복 101 / 351 | 시간 57[s] | 손실 0.03\n",
            "| 에폭 5 |  반복 121 / 351 | 시간 68[s] | 손실 0.03\n",
            "| 에폭 5 |  반복 141 / 351 | 시간 79[s] | 손실 0.02\n",
            "| 에폭 5 |  반복 161 / 351 | 시간 90[s] | 손실 0.02\n",
            "| 에폭 5 |  반복 181 / 351 | 시간 101[s] | 손실 0.02\n",
            "| 에폭 5 |  반복 201 / 351 | 시간 112[s] | 손실 0.02\n",
            "| 에폭 5 |  반복 221 / 351 | 시간 124[s] | 손실 0.02\n",
            "| 에폭 5 |  반복 241 / 351 | 시간 135[s] | 손실 0.02\n",
            "| 에폭 5 |  반복 261 / 351 | 시간 146[s] | 손실 0.01\n",
            "| 에폭 5 |  반복 281 / 351 | 시간 157[s] | 손실 0.01\n",
            "| 에폭 5 |  반복 301 / 351 | 시간 168[s] | 손실 0.01\n",
            "| 에폭 5 |  반복 321 / 351 | 시간 180[s] | 손실 0.01\n",
            "| 에폭 5 |  반복 341 / 351 | 시간 191[s] | 손실 0.01\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 99.760%\n",
            "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 21 / 351 | 시간 11[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 41 / 351 | 시간 22[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 61 / 351 | 시간 33[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 81 / 351 | 시간 44[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 101 / 351 | 시간 55[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 121 / 351 | 시간 66[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 141 / 351 | 시간 77[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 161 / 351 | 시간 88[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 181 / 351 | 시간 99[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 201 / 351 | 시간 110[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 221 / 351 | 시간 121[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 241 / 351 | 시간 132[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 261 / 351 | 시간 143[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 281 / 351 | 시간 154[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 301 / 351 | 시간 166[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 321 / 351 | 시간 177[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 341 / 351 | 시간 188[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 21 / 351 | 시간 11[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 41 / 351 | 시간 23[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 61 / 351 | 시간 34[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 81 / 351 | 시간 45[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 101 / 351 | 시간 56[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 121 / 351 | 시간 68[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 141 / 351 | 시간 79[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 161 / 351 | 시간 90[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 181 / 351 | 시간 102[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 201 / 351 | 시간 114[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 221 / 351 | 시간 126[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 241 / 351 | 시간 138[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 261 / 351 | 시간 150[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 281 / 351 | 시간 162[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 301 / 351 | 시간 174[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 321 / 351 | 시간 185[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 341 / 351 | 시간 197[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 21 / 351 | 시간 11[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 41 / 351 | 시간 23[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 61 / 351 | 시간 34[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 81 / 351 | 시간 46[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 101 / 351 | 시간 57[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 121 / 351 | 시간 68[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 141 / 351 | 시간 79[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 161 / 351 | 시간 90[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 181 / 351 | 시간 102[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 201 / 351 | 시간 113[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 221 / 351 | 시간 124[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 241 / 351 | 시간 136[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 261 / 351 | 시간 147[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 281 / 351 | 시간 158[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 301 / 351 | 시간 169[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 321 / 351 | 시간 181[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 341 / 351 | 시간 192[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 21 / 351 | 시간 12[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 41 / 351 | 시간 23[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 61 / 351 | 시간 35[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 81 / 351 | 시간 47[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 101 / 351 | 시간 59[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 121 / 351 | 시간 71[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 141 / 351 | 시간 84[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 161 / 351 | 시간 96[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 181 / 351 | 시간 108[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 201 / 351 | 시간 121[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 221 / 351 | 시간 133[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 241 / 351 | 시간 146[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 261 / 351 | 시간 158[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 281 / 351 | 시간 171[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 301 / 351 | 시간 183[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 321 / 351 | 시간 195[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 341 / 351 | 시간 207[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 21 / 351 | 시간 12[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 41 / 351 | 시간 25[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 61 / 351 | 시간 37[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 81 / 351 | 시간 49[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 101 / 351 | 시간 62[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 121 / 351 | 시간 74[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 141 / 351 | 시간 86[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 161 / 351 | 시간 98[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 181 / 351 | 시간 110[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 201 / 351 | 시간 122[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 221 / 351 | 시간 134[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 241 / 351 | 시간 147[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 261 / 351 | 시간 159[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 281 / 351 | 시간 171[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 301 / 351 | 시간 183[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 321 / 351 | 시간 195[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 341 / 351 | 시간 207[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "jnE79_FR7IJ7",
        "outputId": "aff24d0e-ba9e-43e6-8653-cb6432f61aed"
      },
      "source": [
        "# 그래프 그리기\n",
        "x = np.arange(len(acc_list))\n",
        "plt.plot(x, acc_list, c='k', marker='o',label='attention')\n",
        "plt.plot(x, acc_list_seq, c='r', marker='o', ls='--', label='seq2seq')\n",
        "plt.plot(x, acc_list_peeky, c='b', marker='o', ls=':', label='seq2seq+peeky')\n",
        "plt.legend(loc=0)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(-0.05, 1.05)\n",
        "plt.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}